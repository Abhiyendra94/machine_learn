{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import data\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "early_stopping = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training images\n",
      "Loading dogs files (Index: 0)\n",
      "Loading cats files (Index: 1)\n"
     ]
    }
   ],
   "source": [
    "image_size=128\n",
    "train_path='/Users/abhiyendragahlot/Downloads/Convolutional_Neural_Networks/dataset/training_set'\n",
    "classes=['dogs','cats']\n",
    "data=data.read_train_sets(train_path=train_path,image_size=image_size,validation_size=0.16,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape,stddev=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biases(num):\n",
    "    return tf.Variable(tf.constant(0.05,shape=[num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(inpu,numinput,numoutfil):\n",
    "    shape=[3,3,numinput,numoutfil]\n",
    "    w=weights(shape)\n",
    "    b=biases(numoutfil)\n",
    "    laya=tf.nn.conv2d(input=inpu,filter=w,strides=[1,1,1,1],padding='SAME')\n",
    "    laya=laya+b\n",
    "    laya=tf.nn.max_pool(value=laya,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    laya=tf.nn.relu(laya)\n",
    "    return laya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(inpu):\n",
    "    shape=inpu.get_shape()\n",
    "    a = int(shape[1]*shape[2]*shape[3])\n",
    "    \n",
    "    # Reshpe the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # the total size of the tensor is unchanged from the reshaping.\n",
    "    laya = tf.reshape(inpu, [-1, a])\n",
    "\n",
    "    return laya,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fclayer(inpu,no_inputs,no_neurons,relu=True):\n",
    "    w=weights([no_inputs,no_neurons])\n",
    "    b=biases(no_neurons)\n",
    "    ans=(tf.matmul(inpu,w)+b)\n",
    "    if relu:\n",
    "        ans=tf.nn.relu(ans)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,shape=[None,128*128*3])\n",
    "x_image = tf.reshape(x, [-1, 128, 128, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue=tf.placeholder(tf.float32,shape=[None,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_cls=tf.arg_max(ytrue,dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=conv(inpu=x_image,numinput=3,numoutfil=32)\n",
    "layer=conv(inpu=layer,numinput=32,numoutfil=32)\n",
    "layer=conv(inpu=layer,numinput=32,numoutfil=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer,numfeatures=flatten(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=fclayer(inpu=layer,no_inputs=numfeatures,no_neurons=128,relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=fclayer(inpu=layer,no_inputs=128,no_neurons=2,relu=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=tf.nn.softmax(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(ypred, dimension=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer,\n",
    "                                                        labels=ytrue)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "session=tf.Session()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_batch_size = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
    "    # Calculate the accuracy on the training-set.\n",
    "    \n",
    "    val_acc = session.run(accuracy, feed_dict=feed_dict_validate)\n",
    "   # msg = \"Epoch {0} ---, Validation Accuracy: {2:>6.1%}, Validation Loss: {3:.3f}\"\n",
    "    print(epoch + 1,val_acc, val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 0\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(train_batch_size)\n",
    "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(train_batch_size)\n",
    "\n",
    "        # Convert shape from [num examples, rows, columns, depth]\n",
    "        # to [num examples, flattened image shape]\n",
    "\n",
    "        #x_batch = x_batch.reshape(train_batch_size, img_size_flat)\n",
    "        #x_valid_batch = x_valid_batch.reshape(train_batch_size, img_size_flat)\n",
    "        x_batch = x_batch.reshape(train_batch_size, 128*128*3)\n",
    "        x_valid_batch = x_valid_batch.reshape(train_batch_size, 128*128*3)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                           ytrue: y_true_batch}\n",
    "        \n",
    "        feed_dict_validate = {x: x_valid_batch,\n",
    "                              ytrue: y_valid_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "        \n",
    "\n",
    "        # Print status epoch (defined as full pass through training dataset).\n",
    "        if i % int(data.train.num_examples/32) == 0: \n",
    "            val_loss = session.run(cost, feed_dict=feed_dict_validate)\n",
    "            epoch = int(i / int(data.train.num_examples/32))\n",
    "            \n",
    "            print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss)\n",
    "            \n",
    "            if early_stopping:    \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience = 0\n",
    "                else:\n",
    "                    patience += 1\n",
    "\n",
    "                if patience == early_stopping:\n",
    "                    break\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print th time-usage.\n",
    "    print(\"Time elapsed: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize(9000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
